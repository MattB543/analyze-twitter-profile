#!/usr/bin/env python3
"""twitter_to_llm.py

Combine parsing of Tweets, Likes, and Bookmarks exported from Twitter/X into
three plain‚Äëtext files (`tweets_for_llm.txt`, `likes_for_llm.txt`,
`bookmarks_for_llm.txt`) that are easy for large‚Äëlanguage models (LLMs) to consume.

Usage
-----
    python twitter_to_llm.py [options]
    
    # Use GUI folder picker (default)
    python twitter_to_llm.py
    
    # Specify folder via command line
    python twitter_to_llm.py --folder /path/to/twitter/data
    
    # Specify your Twitter handle for self-identification
    python twitter_to_llm.py --folder /path/to/data --self-handle yourusername
    
    # Specify your Twitter user ID directly
    python twitter_to_llm.py --folder /path/to/data --self-id 123456789
    
    # Use environment variable for handle
    export MY_TWITTER_HANDLE=yourusername
    python twitter_to_llm.py --folder /path/to/data

The script will find Twitter data files in the selected folder:
- tweets_*.jsonl (from Firefox extension or Twitter export)
- likes_*.jsonl (from Firefox extension or Twitter export)
- bookmarks_*.jsonl (from Firefox extension or Twitter export)
- replies_*.jsonl (from Firefox extension or Twitter export)
- parents.json (optional, generated by hydrate_parents_api.py)

Self-Identification
-------------------
For accurate labeling of your own content as "@me":
1. --self-id: Directly specify your Twitter user ID
2. --self-handle: Specify your Twitter handle (without @)  
3. MY_TWITTER_HANDLE environment variable
4. Auto-detection from tweets file (when available)
5. Fallback analysis of likes/bookmarks (when handle provided)

Output files are written to the same selected folder.

The script:
* reads JSONL files exported by the Firefox extension or Twitter export,
* converts each structure into a minimal, readable text representation,
* tries to include the parent tweet when you replied / quote‚Äëtweeted, when that
  parent tweet is available in your likes file (handy context for an LLM).

Dependencies
------------
Python 3.10+ with the following external packages:
- requests (for URL metadata fetching)
- beautifulsoup4 (for HTML parsing)  
- google-genai (for image captioning via Gemini API)
- python-dotenv (for loading .env file with API keys - optional)
- tkinter (for GUI folder picker - may not be available in headless environments)

Environment Setup
-----------------
Create a .env file in your project directory with:
    GEMINI_API_KEY=your_api_key_here

Alternatively, set the GEMINI_API_KEY environment variable manually.
"""

import json
import re
import sys
import tkinter as tk
import pathlib
import csv
import requests
import mimetypes
import hashlib
import time
import argparse
import urllib.parse
import socket
import ipaddress
import os
import html
from datetime import datetime
from pathlib import Path
from tkinter import filedialog, messagebox
from typing import Any, Dict, List
try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = None
try:
    from google import genai
    from google.genai import types
except Exception as e:
    genai = None
    types = None
    _genai_import_error = e
from bs4 import BeautifulSoup

CLIENT = None

def get_client():
    """Get Gemini client, initializing on first call."""
    global CLIENT
    if CLIENT is None:
        if genai is None:
            raise RuntimeError(f"Google GenAI not available: {_genai_import_error}")
        
        # Load environment variables from .env file if available
        if load_dotenv is not None:
            load_dotenv()
        elif not os.getenv('GEMINI_API_KEY'):
            print("‚ö†Ô∏è  python-dotenv not installed. Install with: pip install python-dotenv")
            print("üí°  Alternatively, set GEMINI_API_KEY environment variable manually")
        
        CLIENT = genai.Client()  # reads GEMINI_API_KEY from environment
    return CLIENT

# Regex for Twitter image URLs (only pbs.twimg.com URLs with query params)
# Note: We deliberately exclude t.co URLs from this regex because we rely on 
# media_mappings from entities data to resolve t.co -> pbs.twimg.com URLs.
# All parsers (tweets, likes, bookmarks) populate media_mappings from extended_entities.media
IMG_RE = re.compile(r"https://pbs\.twimg\.com/(?:media/\S+(?:\?format=(?:jpe?g|png|webp)|\.(?:jpe?g|png|webp))|amplify_video_thumb/\S+)")
# Regex for general external URLs (excluding Twitter image URLs, stop at common punctuation)
URL_RE = re.compile(r"https?://(?!pbs\.twimg\.com|t\.co)[^\s)\],>\"']+")

# Common file extensions to exclude (will filter these out separately)
EXCLUDE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.mp4', '.pdf', '.zip', '.tar', '.gz', '.rar', '.exe', '.dmg'}

# --------------------------------------------------------------------------- #
#  Image Caption Processing Controls                                          #
# --------------------------------------------------------------------------- #

# Enable/disable image caption processing for each file type
# Set to False to skip image captioning for specific file types to save time/API costs
# Note: Disabling captions means images will appear as raw URLs instead of [image](description)
ENABLE_CAPTIONS_TWEETS = True
ENABLE_CAPTIONS_LIKES = False  # Disable by default to avoid costly captioning for likes
ENABLE_CAPTIONS_BOOKMARKS = True
ENABLE_CAPTIONS_REPLIES = False  # Disable captioning for replies/parent context by default

# --------------------------------------------------------------------------- #
#  Helpers                                                                    #
# --------------------------------------------------------------------------- #

def find_files_in_folder(folder: Path) -> tuple[Path | None, Path | None, Path | None, Path | None, Path | None]:
    """Find cleaned_*.jsonl files and parents.json files in the given folder.
    
    Returns:
        Tuple of (tweets_file, likes_file, bookmarks_file, replies_file, parents_file) or None if not found
    """
    tweets_file = None
    likes_file = None
    bookmarks_file = None
    replies_file = None
    parents_file  = None
    
    # Look for cleaned_tweets_*.jsonl (from hydrate_parents_api.py cleaning)
    for file in folder.glob("cleaned_tweets_*.jsonl"):
        tweets_file = file
        break  # Take the first one found
    
    # Look for cleaned_likes_*.jsonl (from hydrate_parents_api.py cleaning)
    for file in folder.glob("cleaned_likes_*.jsonl"):
        likes_file = file
        break  # Take the first one found
    
    # Look for cleaned_bookmarks_*.jsonl
    for file in folder.glob("cleaned_bookmarks_*.jsonl"):
        bookmarks_file = file
        break  # Take the first one found
    
    # Look for cleaned_replies_*.jsonl
    for file in folder.glob("cleaned_replies_*.jsonl"):
        replies_file = file
        break
    
    # Look for parents.json
    parents_path = folder / "parents.json"
    if parents_path.exists():
        parents_file = parents_path
    
    return tweets_file, likes_file, bookmarks_file, replies_file, parents_file


# --------------------------------------------------------------------------- #
#  Tweets & Likes                                                             #
# --------------------------------------------------------------------------- #

def parse_twitter_jsonl(file_path: Path, file_type: str, self_ids: set = None) -> tuple[List[Dict[str, Any]], Dict[str, str], Dict[str, str], Dict[str, str], Dict[str, Dict[str, Any]]]:
    """
    Unified parser for CLEANED tweets, likes, and bookmarks JSONL files.
    """
    records = []
    text_lookup = {}
    all_media_mappings = {} # This can be simplified or removed, see below
    all_url_mappings = {}   # This can be simplified or removed, see below
    meta_by_id = {}

    if self_ids is None:
        self_ids = set()

    with file_path.open('r', encoding='utf-8') as infile:
        for line_no, line in enumerate(infile, 1):
            if not line.strip():
                continue
            try:
                obj = json.loads(line)
                tweet_id = obj.get("id")
                if not tweet_id:
                    continue

                # The text is already resolved and clean
                text = obj.get("text", "")
                text_lookup[tweet_id] = text

                # The URLs are already extracted
                # We will handle these later, but we can collect them
                # Note: The old script expected mappings (t.co -> expanded),
                # but the new format gives a flat list of expanded URLs.
                # We will adapt the downstream logic to handle this.

                # Populate metadata for context chains
                author_id = obj.get("author_id")
                interaction_type = obj.get("interaction_type")
                linked_id = obj.get("linked_tweet_id")

                reply_to_id = linked_id if interaction_type == 'reply' else None
                quote_id = linked_id if interaction_type == 'quote_tweet' else None

                meta_by_id[tweet_id] = {
                    "reply_to": reply_to_id,
                    "quoted": quote_id,
                    "screen_name": obj.get("screen_name"),
                    "author_id": author_id,
                    "reply_to_user": obj.get("reply_to_screen_name"),
                    "source": file_type
                }

                # Create the unified record for export_unified_text
                is_retweet = interaction_type == 'retweet'

                record = {
                    "id": tweet_id,
                    "created_at": obj.get("created_at", ""),
                    "text": text,
                    "full_text": text, # For compatibility with existing logic
                    "author_id": author_id,
                    "authored_by_me": (author_id and author_id in self_ids),
                    "screen_name": obj.get("screen_name"),
                    "is_retweet": is_retweet,
                    "retweeted_user": obj.get("retweeted_screen_name") if is_retweet else None,
                    "retweeted_text": obj.get("retweeted_text") if is_retweet else None,
                    "is_reply": bool(reply_to_id),
                    "quoted_tweet_id": quote_id,
                    "reply_to_tweet_id": reply_to_id,
                    "reply_to_user": obj.get("reply_to_screen_name"),
                    "source": file_type,
                    # Add the new URL lists to the record
                    "urls": obj.get("urls", []),
                    "media_urls": obj.get("media_urls", [])
                }
                records.append(record)

            except json.JSONDecodeError:
                print(f"‚ö†Ô∏è  Skipping malformed JSON on line {line_no} in {file_path.name}", file=sys.stderr)
                continue

    # The old return values for URL mappings are no longer built this way.
    # We return empty dicts for compatibility but will adapt the main loop.
    return records, text_lookup, {}, {}, meta_by_id


def load_parents_json(parents_file: Path) -> tuple[Dict[str, str], Dict[str, str], Dict[str, Dict[str, Any]]]:
    """Load parent tweets from parents.json and convert to lookup formats.
    
    Returns:
        Tuple of (parent_lookup, parent_url_mappings, parent_metadata)
    """
    try:
        with parents_file.open('r', encoding='utf-8') as f:
            parents_data = json.load(f)
        
        # Convert Twitter API v2 format to our lookup format
        parent_lookup = {}
        parent_url_mappings = {}
        parent_metadata = {}
        
        for tweet_id, tweet_data in parents_data.items():
            # Extract text from Twitter API response - handle both v2 and v1.1 formats
            text = (tweet_data.get('text')
                    or tweet_data.get('legacy', {}).get('full_text')
                    or tweet_data.get('legacy', {}).get('text')
                    or "")
            if text:
                parent_lookup[tweet_id] = text
            
            # Extract URL mappings from entities when available - handle both v2 and v1.1 formats
            entities = (tweet_data.get('entities')
                        or tweet_data.get('legacy', {}).get('entities')
                        or {})
            for url_entity in entities.get('urls', []):
                short = url_entity.get('url', '')
                expanded = url_entity.get('expanded_url', '')
                if short and expanded:
                    parent_url_mappings[short] = expanded
            
            # Extract relationship metadata for context chain building
            referenced_tweets = tweet_data.get('referenced_tweets', [])
            reply_to = ""
            quoted = ""
            
            for ref in referenced_tweets:
                if ref.get('type') == 'replied_to':
                    reply_to = ref.get('id', '')
                elif ref.get('type') == 'quoted':
                    quoted = ref.get('id', '')
            
            # ISSUE #1 FIX: Fallback to legacy fields if referenced_tweets is empty or missing relationships
            if not reply_to or not quoted:
                legacy = tweet_data.get('legacy', {})
                if not reply_to:
                    legacy_reply_id = legacy.get('in_reply_to_status_id_str')
                    if legacy_reply_id:
                        reply_to = str(legacy_reply_id)
                
                if not quoted:
                    legacy_quoted_id = legacy.get('quoted_status_id_str')
                    if legacy_quoted_id:
                        quoted = str(legacy_quoted_id)
            
            # Extract author information with a more robust, multi-step process
            author_username = ""
            author_id = None

            # --- Step 1: Find the Author ID from multiple possible locations ---
            author_id = tweet_data.get('author_id')
            if not author_id:
                author_id = tweet_data.get('legacy', {}).get('user_id_str')

            # --- Step 2: Use the Author ID to find the screen name ---
            if author_id:
                # Method A: Look in the API v2 `includes` object (most reliable)
                includes = tweet_data.get('includes', {})
                users = includes.get('users', [])
                for user in users:
                    if user.get('id') == author_id:
                        author_username = user.get('username', '')
                        break

            # --- Step 3: If screen name is still not found, try other fallbacks ---
            if not author_username:
                # Method B: Check the GraphQL-style `core` structure
                user_result = tweet_data.get("core", {}).get("user_results", {}).get("result", {})
                if user_result:
                    author_username = user_result.get("core", {}).get("screen_name") or \
                                      user_result.get("legacy", {}).get("screen_name")

            if not author_username:
                # Method C: Check for a legacy top-level `user` object
                user_object = tweet_data.get("user", {})
                if user_object:
                    author_username = user_object.get("screen_name")

            if not author_username:
                # Method D: Check for a user object inside the `legacy` object
                legacy_user_object = tweet_data.get("legacy", {}).get("user", {})
                if legacy_user_object:
                    author_username = legacy_user_object.get("screen_name")

            if not author_username:
                # Method E: Check for TwitterAPI.io format - author.userName
                author_object = tweet_data.get("author", {})
                if author_object:
                    author_username = author_object.get("userName")
            
            # Also get reply_to_user from legacy if available
            reply_to_user = ""
            if reply_to:
                legacy = tweet_data.get('legacy', {})
                reply_to_user = legacy.get('in_reply_to_screen_name', '')
            
            # Store metadata for relationship traversal
            parent_metadata[tweet_id] = {
                "reply_to": reply_to,
                "quoted": quoted,
                "screen_name": author_username,
                "reply_to_user": reply_to_user,
                "source": "parents"
            }
        
        print(f"üìñ  Loaded {len(parent_lookup)} parent tweets from {parents_file.name}")
        relationships_count = len([m for m in parent_metadata.values() if m['reply_to'] or m['quoted']])
        if relationships_count > 0:
            print(f"üîó  Extracted {relationships_count} relationship metadata from parents")
        if parent_url_mappings:
            print(f"üìñ  Extracted {len(parent_url_mappings)} URL mappings from parent tweets")
        return parent_lookup, parent_url_mappings, parent_metadata
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to load parent tweets: {e}")
        return {}, {}, {}



def get_thread_context(tweet_id: str, tweet_lookup: Dict[str, str], max_depth: int = 3, visited: set = None) -> List[str]:
    """
    Get thread context for a tweet, following parent relationships up to max_depth.
    
    Args:
        tweet_id: The starting tweet ID
        tweet_lookup: Dictionary mapping tweet IDs to tweet content
        max_depth: Maximum depth to traverse (default 3 to prevent infinite chains)
        visited: Set of already visited tweet IDs to prevent cycles
    
    Returns:
        List of tweet texts in chronological order (oldest first)
    """
    if visited is None:
        visited = set()
    
    if tweet_id in visited or max_depth <= 0:
        return []
    
    visited.add(tweet_id)
    context = []
    
    # Get the current tweet content
    current_content = tweet_lookup.get(tweet_id, "")
    if current_content:
        context.append(current_content)
    
    # Note: For deeper thread context, we'd need parent relationships
    # The current implementation is already limited to 1 level, which is good
    
    return context


def build_context_chain(tweet_id: str, meta_by_id: Dict[str, Dict[str, Any]], text_lookup: Dict[str, str], 
                       depth: int = 3, visited: set | None = None) -> List[Dict[str, Any]]:
    """
    Build a context chain for a tweet with replies and quotes.
    Handles both reply and quote relationships simultaneously.
    
    Args:
        tweet_id: The starting tweet ID
        meta_by_id: Dictionary mapping tweet IDs to metadata (reply_to, quoted, screen_name)
        text_lookup: Dictionary mapping tweet IDs to tweet text
        depth: Maximum depth to traverse (default 3 to prevent infinite chains)
        visited: Set of already visited tweet IDs to prevent cycles
    
    Returns:
        List of context items in chronological order: [reply_chain_oldest_first, quoted_content, current_tweet], each containing:
        {"type": "tweet/quoted/quoted_unavailable", "id": str, "screen_name": str, "text": str}
    """
    if visited is None:
        visited = set()
    
    if not tweet_id or depth <= 0 or tweet_id in visited:
        return []
    
    visited.add(tweet_id)
    meta = meta_by_id.get(tweet_id, {})
    text = text_lookup.get(tweet_id, "")
    screen_name = meta.get("screen_name", "unknown")
    reply_to_user = meta.get("reply_to_user", "")
    
    chain = []
    
    # 1. First, recurse to reply parent (oldest tweets come first)
    parent_id = meta.get("reply_to")
    if parent_id and parent_id in text_lookup and parent_id not in visited:
        parent_chain = build_context_chain(parent_id, meta_by_id, text_lookup, depth - 1, visited)
        chain.extend(parent_chain)
    
    # 2. If this tweet quotes another tweet, add quoted content first (chronological order)
    quoted_id = meta.get("quoted")
    if quoted_id:
        if quoted_id in text_lookup and quoted_id not in visited:
            # First get the quoted tweet's own context (its reply chain, if any)
            quoted_chain = build_context_chain(quoted_id, meta_by_id, text_lookup, depth - 1, visited)
            
            # Add the entire quoted chain (which includes the quoted tweet itself)
            # This ensures proper chronological ordering within the quoted context
            for item in quoted_chain:
                if item["id"] == quoted_id:
                    # Mark the actual quoted tweet with special type
                    item["type"] = "quoted"
                chain.append(item)
        else:
            # Quoted tweet is not available - add placeholder with marker
            chain.append({
                "type": "quoted_unavailable",
                "id": quoted_id,
                "screen_name": "unknown",
                "text": f"[Quoted tweet {quoted_id} not available]",
                "reply_to_user": ""
            })
    
    # 3. Add the current tweet last (after quoted content for chronological order)
    chain.append({
        "type": "tweet",
        "id": tweet_id,
        "screen_name": screen_name,
        "text": text,
        "reply_to_user": reply_to_user
    })
    
    return chain


def strip_trailing_quote_url_aggressive(text: str, quoted_id: str, url_mappings: Dict[str, str] = None, url_to_caption: Dict[str, str] = None) -> str:
    """
    Remove trailing t.co URL that likely points to a quoted tweet.
    Uses both URL mappings and heuristic patterns for more aggressive stripping.
    
    ISSUE #7 FIX: Improved heuristics that handle cases where quoted tweet attachments
    don't appear as t.co in full_text, and avoid stripping needed media links.
    """
    import re
    
    # First try exact matching with URL mappings
    if url_mappings and quoted_id:
        # Find trailing t.co tokens and check if they map to the quoted tweet
        m = re.search(r'(https://t\.co/\w+)[\s\.\!\?]*$', text)
        if m:
            short_url = m.group(1)
            expanded = url_mappings.get(short_url, "")
            
            # Check if the expanded URL contains the quoted tweet ID
            if expanded and quoted_id in expanded:
                return text[:m.start()].rstrip()
    
    # Improved heuristic: be more conservative about stripping
    if quoted_id:
        # Look for trailing t.co URLs, but be more careful
        m = re.search(r'\s+(https://t\.co/\w+)[\s\.\!\?]*$', text)
        if m:
            tco_url = m.group(1)
            
            # Don't strip if:
            # 1. It's a media URL (has caption)
            # 2. It's the only URL in the tweet (likely important content)
            # 3. It's immediately after specific keywords that suggest it's content, not a quote link
            
            if url_to_caption and tco_url in url_to_caption:
                return text  # Don't strip media URLs
            
            # Check if this is the only URL in the tweet
            all_tco_urls = re.findall(r'https://t\.co/\w+', text)
            if len(all_tco_urls) == 1:
                # If it's the only URL, be more conservative
                # Only strip if there's clear evidence this is a quote URL
                # Look for tweet text patterns that suggest this is added quote link
                text_before_url = text[:m.start()].strip()
                if len(text_before_url) > 20:  # Has substantial content before URL
                    return text[:m.start()].rstrip()
                else:
                    return text  # Don't strip if tweet is mostly just the URL
            else:
                # Multiple URLs - safer to strip trailing one if it looks like quote
                return text[:m.start()].rstrip()
    
    return text


def export_unified_text(records: List[Dict[str, Any]],
                        tweet_lookup: Dict[str, str],
                        outfile: Path,
                        meta_by_id: Dict[str, Dict[str, Any]],
                        url_to_caption: Dict[str, str] = None,
                        url_to_meta: Dict[str, str] = None,
                        url_mappings: Dict[str, str] = None) -> None:
    """Unified export function with recursive context for all record types."""
    
    def process_text(text: str) -> str:
        """Apply HTML unescaping, URL expansion, image captions, and metadata to text."""
        # First, convert any HTML entities (e.g., &gt;, &amp;) back to their symbols
        text = html.unescape(text)

        # Then apply the usual transformations
        if url_mappings:
            text = expand_short_urls(text, url_mappings)
        if url_to_caption:
            text = replace_images_with_captions(text, url_to_caption)
        if url_to_meta:
            text = replace_urls_with_meta(text, url_to_meta)
        return text.replace("\r", "")
    
    def strip_trailing_quote_url(text: str, quoted_id: str, url_mappings: Dict[str, str]) -> str:
        """Remove trailing t.co URL that specifically points to the quoted tweet."""
        # Use the new aggressive stripping function
        return strip_trailing_quote_url_aggressive(text, quoted_id, url_mappings, url_to_caption)
    
    def append_urls_and_media(f, record: Dict[str, Any]) -> None:
        """Append media and links from record to the output file."""
        # Append media urls with captions
        media_urls = record.get("media_urls", [])
        for media_url in media_urls:
            caption = url_to_caption.get(media_url) if url_to_caption else "Image"
            if not caption or caption.startswith("ERROR"):
                caption = "Image not captioned"
            safe_caption = caption.replace(")", r"\)").replace("\n", " ")
            f.write(f"\n[image]({safe_caption})")

        # Append external urls with metadata
        external_urls = record.get("urls", [])
        for url in external_urls:
            enhanced_meta = url_to_meta.get(url, url) if url_to_meta else url
            f.write(f"\n[link]({enhanced_meta})")
    
    # Track tweets that have already been displayed in context chains to prevent duplication
    already_displayed = set()
    
    with outfile.open("w", encoding="utf-8") as f:
        # Determine the tag name from the filename
        file_type = outfile.stem.split('_')[0]  # Extract 'tweets', 'likes', or 'bookmarks' from filename
        f.write(f"<{file_type}>\n")
        
        # Track if we've written any content for separator logic
        content_written = False
        
        for i, record in enumerate(records):
            tweet_id = record.get("id")
            source = record.get("source", "")
            
            # Skip if this tweet was already displayed as part of a previous context chain
            if tweet_id in already_displayed:
                continue
            
            # Check if this record has reply/quote relationships or is a retweet
            is_reply = record.get("is_reply", False)
            is_quote = bool(record.get("quoted_tweet_id"))
            is_retweet = record.get("is_retweet", False)
            
            # Handle retweets with proper labeling
            if is_retweet:
                # Add separator if we've written content before
                if content_written:
                    f.write("\n---\n")
                
                # For retweets, always prefer retweeted_text, then fall back to main text
                main_text = record.get("retweeted_text") or record.get("text", "")
                main_text = process_text(main_text)
                
                if record.get("retweeted_user"):
                    f.write(f"RT @{record['retweeted_user']}:\n")
                else:
                    f.write("RT (unknown):\n")
                f.write(main_text)
                
                # Append URLs and media for retweets
                append_urls_and_media(f, record)
                
                # Mark this tweet as displayed
                already_displayed.add(tweet_id)
                content_written = True
            
            elif is_reply or is_quote:
                # Add separator if we've written content before
                if content_written:
                    f.write("\n---\n")
                
                # Build full context chain for this tweet (handles both reply AND quote)
                context_chain = build_context_chain(tweet_id, meta_by_id, tweet_lookup)
                
                if context_chain:
                    print(f"üîó  Built context chain: {len(context_chain)} items for tweet {tweet_id} (reply: {is_reply}, quote: {is_quote})", file=sys.stderr)
                    
                    # Track if we've written the current user's tweet yet
                    user_tweet_written = False
                    
                    for j, context_item in enumerate(context_chain):
                        ctx_screen_name = context_item.get("screen_name", "unknown")
                        ctx_text = context_item.get("text", "")
                        ctx_type = context_item.get("type", "tweet")
                        ctx_id = context_item.get("id", "")
                        
                        # Strip quote URLs before processing text (but not for unavailable quotes)
                        if ctx_type != "quoted_unavailable":
                            quoted_id = meta_by_id.get(ctx_id, {}).get("quoted")
                            if quoted_id and ctx_type != "quoted":
                                ctx_text = strip_trailing_quote_url(ctx_text, quoted_id, url_mappings or {})
                        
                        # Process text (URL expansion, captions, metadata) - but not for unavailable quotes
                        if ctx_type != "quoted_unavailable":
                            ctx_text = process_text(ctx_text)
                        
                        # Truncate very long context tweets (but not unavailable quote markers)
                        if ctx_type != "quoted_unavailable" and len(ctx_text) > 500:
                            ctx_text = ctx_text[:1500] + "... [truncated]"
                        
                        # Format with consistent @author: format for clear conversation flow
                        label = f"@{ctx_screen_name}" if ctx_screen_name and ctx_screen_name != "unknown" else "@unknown"

                        if ctx_id == tweet_id:
                            # This is the tweet being processed (could be user's or someone else's)
                            user_tweet_written = True
                            if record.get("authored_by_me"):
                                f.write("@me:\n")
                            else:
                                f.write(f"{label}:\n")
                            f.write(ctx_text)
                            # Append URLs and media for the main tweet in context chain
                            append_urls_and_media(f, record)
                        elif ctx_type == "quoted":
                            f.write(f"Quoted tweet ({label}):\n{ctx_text}\n\n")
                        elif ctx_type == "quoted_unavailable":
                            f.write(f"Quoted tweet (unavailable):\n{ctx_text}\n\n")
                        else:
                            f.write(f"{label}:\n{ctx_text}\n\n")
                    
                    # Mark all tweets in this context chain as displayed to prevent future duplication
                    for context_item in context_chain:
                        ctx_id = context_item.get("id")
                        if ctx_id:
                            already_displayed.add(ctx_id)
                    content_written = True
                    
                    # If somehow the user's tweet wasn't written (shouldn't happen with new logic)
                    if not user_tweet_written:
                        main_text = record.get("text" if source in ("tweets", "replies") else "full_text", "")
                        main_text = process_text(main_text)
                        if source in ("tweets", "replies"):
                            f.write("@me:\n")
                        else:
                            # This should not happen in tweets_for_llm.txt since source should always be "tweets"
                            screen_name = record.get("screen_name", "")
                            if screen_name:
                                f.write(f"@{screen_name}:\n")
                            else:
                                f.write("@unknown:\n")
                        f.write(main_text)
                        
                        # Append URLs and media for fallback case
                        append_urls_and_media(f, record)
                        
                        # Mark this tweet as displayed
                        already_displayed.add(tweet_id)
                        content_written = True
                
                else:
                    # No context chain built, but has relationships - fallback
                    main_text = record.get("text" if source in ("tweets", "replies") else "full_text", "")
                    
                    # Handle unavailable quoted tweets in fallback case
                    quoted_tweet_id = record.get("quoted_tweet_id")
                    if quoted_tweet_id and quoted_tweet_id not in tweet_lookup:
                        # Strip trailing quote URL since quoted tweet is not available
                        main_text = strip_trailing_quote_url_aggressive(main_text, quoted_tweet_id, url_mappings or {}, url_to_caption)
                    
                    main_text = process_text(main_text)
                    if source in ("tweets", "replies"):
                        f.write("@me:\n")
                    else:
                        screen_name = record.get("screen_name", "")
                        # Fallback to metadata if screen_name is not in the top-level record
                        if not screen_name:
                            meta = meta_by_id.get(record.get("id"), {})
                            screen_name = meta.get("screen_name", "unknown")
                        f.write(f"@{screen_name}:\n")
                    f.write(main_text)
                    
                    # Append URLs and media for fallback case
                    append_urls_and_media(f, record)
                    
                    # Add unavailable quote marker if needed
                    if quoted_tweet_id and quoted_tweet_id not in tweet_lookup:
                        if not main_text.endswith('\n'):
                            f.write("\n")
                        f.write("\nQuoted tweet (unavailable):\n[Quoted tweet {quoted_tweet_id} not available]")
                    
                    # Mark this tweet as displayed
                    already_displayed.add(tweet_id)
                    content_written = True
            
            else:
                # Add separator if we've written content before
                if content_written:
                    f.write("\n---\n")
                
                # No reply/quote relationships - simple format
                if record.get("authored_by_me"):
                    f.write("@me:\n")
                else:
                    screen_name = record.get("screen_name", "")
                    # Fallback to metadata if screen_name is not in the top-level record
                    if not screen_name:
                        meta = meta_by_id.get(record.get("id"), {})
                        screen_name = meta.get("screen_name", "unknown")
                    f.write(f"@{screen_name}:\n")

                # Unify text retrieval
                main_text = record.get("text") or record.get("full_text", "")
                
                # Check for potential undetected quoted tweets with trailing t.co URLs
                # This is a fallback to catch cases where quote detection may have failed
                import re
                trailing_tco_match = re.search(r'\s+(https://t\.co/\w+)[\s\.\!\?]*$', main_text)
                if trailing_tco_match:
                    tco_url = trailing_tco_match.group(1)
                    # Only strip if this t.co URL is NOT a media URL (doesn't have a caption)
                    if not url_to_caption or tco_url not in url_to_caption:
                        # Heuristically strip trailing t.co URLs that might be quote links
                        main_text = main_text[:trailing_tco_match.start()].rstrip()
                
                main_text = process_text(main_text)
                f.write(main_text)
                
                # Append URLs and media for simple tweets
                append_urls_and_media(f, record)
                
                # Mark this tweet as displayed
                already_displayed.add(tweet_id)
                content_written = True
        
        # Close the XML tag
        f.write(f"\n</{file_type}>")




# --------------------------------------------------------------------------- #
#  Bookmarks                                                                  #
# --------------------------------------------------------------------------- #



# --------------------------------------------------------------------------- #
#  URL Metadata Extraction                                                    #
# --------------------------------------------------------------------------- #

def should_fetch_url(url: str, allow_domains: set = None) -> bool:
    """Check if a URL is safe to fetch (SSRF protection)."""
    try:
        parsed = urllib.parse.urlparse(url)
        if parsed.scheme not in ("http", "https"):
            return False
        
        hostname = parsed.hostname
        if not hostname:
            return False
            
        # Check domain allowlist if provided
        if allow_domains and hostname not in allow_domains:
            return False
        
        # Resolve hostname to IP and check for private ranges
        try:
            ip = socket.gethostbyname(hostname)
            ip_obj = ipaddress.ip_address(ip)
            if ip_obj.is_private or ip_obj.is_loopback or ip_obj.is_link_local:
                return False
        except (socket.gaierror, ValueError):
            # If we can't resolve, let it through (will fail on request)
            pass
            
        return True
    except Exception:
        return False


def fetch_url_metadata(url: str, max_retries: int = 1, allow_domains: set = None) -> Dict[str, str]:
    """Fetch meta title and description from a URL with retry logic.
    
    Args:
        url: URL to fetch metadata from
        max_retries: Maximum number of retry attempts
        allow_domains: Set of allowed domains (optional)
    
    Returns:
        Dictionary with 'title' and 'description' keys
    """
    # Safety check
    if not should_fetch_url(url, allow_domains):
        return {
            'title': f"ERROR: URL blocked for security reasons",
            'description': ""
        }
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for attempt in range(max_retries + 1):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            
            # Handle rate limiting with exponential backoff
            if response.status_code == 429:
                if attempt < max_retries:
                    wait_time = (2 ** attempt) * 1  # 1, 2, 4 seconds
                    print(f"‚è≥ Rate limited on {url}, waiting {wait_time}s (attempt {attempt + 1}/{max_retries + 1})")
                    time.sleep(wait_time)
                    continue
                else:
                    return {
                        'title': f"ERROR: Rate limited after {max_retries} retries",
                        'description': ""
                    }
            
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract title
            title = ""
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text().strip()
            
            # Extract meta description
            description = ""
            desc_tag = soup.find('meta', attrs={'name': 'description'})
            if not desc_tag:
                desc_tag = soup.find('meta', attrs={'property': 'og:description'})
            if desc_tag:
                description = desc_tag.get('content', '').strip()
            
            return {
                'title': title,
                'description': description
            }
            
        except requests.exceptions.RequestException as e:
            if attempt < max_retries:
                wait_time = (2 ** attempt) * 1  # 1, 2, 4 seconds
                print(f"‚ö†Ô∏è  Request failed for {url}, retrying in {wait_time}s (attempt {attempt + 1}/{max_retries + 1}): {e}")
                time.sleep(wait_time)
                continue
            else:
                return {
                    'title': f"ERROR: {e}",
                    'description': ""
                }
        except Exception as e:
            return {
                'title': f"ERROR: {e}",
                'description': ""
            }
    
    # Should not reach here, but just in case
    return {
        'title': "ERROR: Unexpected failure",
        'description': ""
    }


def generate_url_metadata_from_texts(texts: List[str], max_urls: int = 1000, allow_domains: set = None) -> Dict[str, str]:
    """Generate metadata for all external URLs found in the given texts.
    
    Args:
        texts: List of text content to scan for URLs
        max_urls: Maximum number of URLs to process (default 1000)
        allow_domains: Set of allowed domains for fetching (optional)
    
    Returns:
        Dictionary mapping URLs to their enhanced format with title and description
    """
    url_to_meta = {}
    all_urls = set()
    
    # Collect all unique external URLs from all texts
    for text in texts:
        urls = URL_RE.findall(text)
        # Filter out URLs with excluded file extensions
        filtered_urls = []
        for url in urls:
            # Check if URL ends with any excluded extension
            url_lower = url.lower()
            is_excluded = any(url_lower.endswith(ext) for ext in EXCLUDE_EXTENSIONS)
            if not is_excluded:
                filtered_urls.append(url)
        all_urls.update(filtered_urls)
    
    # Limit the number of URLs to process
    urls_to_process = list(all_urls)[:max_urls]
    if len(all_urls) > max_urls:
        print(f"‚ö†Ô∏è  Found {len(all_urls)} URLs, limiting to first {max_urls} for processing")
    
    # Generate metadata for each unique URL
    for i, url in enumerate(urls_to_process, 1):
        try:
            metadata = fetch_url_metadata(url, allow_domains=allow_domains)
            title = metadata['title']
            description = metadata['description']
            
            # Create the enhanced format: URL (title - description)
            if title and description:
                enhanced = f"{url} ({title} - {description})"
            elif title:
                enhanced = f"{url} ({title})"
            else:
                enhanced = url  # Keep original if no metadata found
            
            url_to_meta[url] = enhanced
            print(f"‚úÖ  [{i}/{len(urls_to_process)}] Generated metadata for {url}")
        except Exception as e:
            url_to_meta[url] = url  # Keep original on error
            print(f"‚ùå  [{i}/{len(urls_to_process)}] Failed to get metadata for {url}: {e}")
    
    return url_to_meta


def generate_url_metadata_from_urls(urls_to_process: List[str], max_urls: int = 1000, allow_domains: set = None) -> Dict[str, str]:
    """Generate metadata for a list of URLs."""
    url_to_meta = {}
    
    if not urls_to_process:
        return url_to_meta
    
    # Filter out URLs with excluded file extensions
    filtered_urls = []
    for url in urls_to_process:
        url_lower = url.lower()
        is_excluded = any(url_lower.endswith(ext) for ext in EXCLUDE_EXTENSIONS)
        if not is_excluded:
            filtered_urls.append(url)
    
    # Limit the number of URLs to process
    if len(filtered_urls) > max_urls:
        filtered_urls = filtered_urls[:max_urls]
        print(f"‚ö†Ô∏è  Found {len(urls_to_process)} URLs, limiting to first {max_urls} for processing")
    
    # Generate metadata for each unique URL
    for i, url in enumerate(filtered_urls, 1):
        try:
            metadata = fetch_url_metadata(url, allow_domains=allow_domains)
            title = metadata['title']
            description = metadata['description']
            
            # Create the enhanced format: URL (title - description)
            if title and description:
                enhanced = f"{url} ({title} - {description})"
            elif title:
                enhanced = f"{url} ({title})"
            else:
                enhanced = url  # Keep original if no metadata found
            
            url_to_meta[url] = enhanced
            print(f"‚úÖ  [{i}/{len(filtered_urls)}] Generated metadata for {url}")
        except Exception as e:
            url_to_meta[url] = url  # Keep original on error
            print(f"‚ùå  [{i}/{len(filtered_urls)}] Failed to get metadata for {url}: {e}")
    
    return url_to_meta


def expand_short_urls(text: str, url_mappings: Dict[str, str]) -> str:
    """Expand shortened URLs in text using the URL mappings with optimized regex replacement."""
    import re
    if not url_mappings:
        return text
    
    # Sort by length (longest first) to avoid partial replacements
    # Escape special regex characters and create pattern
    pattern = re.compile('|'.join(re.escape(k) for k in sorted(url_mappings, key=len, reverse=True)))
    
    # Single-pass replacement using regex substitution
    return pattern.sub(lambda m: url_mappings[m.group(0)], text)


def replace_urls_with_meta(text: str, url_to_meta: Dict[str, str]) -> str:
    """Replace URLs in text with enhanced format including title and description with optimized regex replacement."""
    import re
    if not url_to_meta:
        return text
    
    # Sort by length (longest first) to avoid partial replacements
    # Escape special regex characters and create pattern
    pattern = re.compile('|'.join(re.escape(k) for k in sorted(url_to_meta, key=len, reverse=True)))
    
    # Single-pass replacement using regex substitution
    return pattern.sub(lambda m: url_to_meta[m.group(0)], text)


def save_url_metadata_csv(url_to_meta: Dict[str, str], out_path: Path):
    """Save URL to metadata mappings as CSV, merging with existing data."""
    # Read existing URL metadata if file exists
    existing_metadata = {}
    if out_path.exists():
        try:
            with out_path.open("r", newline="", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    existing_metadata[row["url"]] = row["enhanced"]
            print(f"üìñ  Loaded {len(existing_metadata)} existing URL metadata from {out_path.name}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to read existing URL metadata: {e}")
    
    # Merge new metadata with existing (new ones take priority)
    all_metadata = existing_metadata.copy()
    all_metadata.update(url_to_meta)
    
    # Convert to rows format
    metadata_rows = []
    for original_url, enhanced in all_metadata.items():
        # Extract title and description from enhanced format
        if " (" in enhanced and enhanced.endswith(")"):
            meta_part = enhanced[enhanced.rfind(" (") + 2:-1]
            if " - " in meta_part:
                title, description = meta_part.split(" - ", 1)
            else:
                title = meta_part
                description = ""
        else:
            title = ""
            description = ""
        
        metadata_rows.append({
            "url": original_url,
            "title": title,
            "description": description,
            "enhanced": enhanced
        })
    
    with out_path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, ["url", "title", "description", "enhanced"])
        writer.writeheader()
        writer.writerows(metadata_rows)
    
    new_count = len(url_to_meta)
    total_count = len(all_metadata)
    print(f"‚úÖ  Saved {total_count} total URL metadata ({new_count} new, {total_count - new_count} existing)")


# --------------------------------------------------------------------------- #
#  Image Captioning                                                           #
# --------------------------------------------------------------------------- #

def describe_image(url: str, media_mappings: Dict[str, str] = None, cache_dir: Path = None, prompt="""You are an expert image analyst creating a summary for a language model that is analyzing social media posts. Your summary must be a single, dense paragraph.

Prioritize in this order:
1.  **Key Text:** Extract the most important text (titles, headlines, key phrases in a meme, data labels on a graph). This is the most critical information.
2.  **Image Type & Subject:** Identify the type of image (e.g., screenshot of an article, infographic, meme, photograph, book cover) and its main subject.
3.  **Core Message or Event:** What is the central message, action, or event? What is the main takeaway at a glance?
4.  **Key Entities:** Mention any important people, products, or organizations shown.

AVOID describing:
- Colors, fonts, or specific layout details (e.g., "two-column layout", "serif font").
- Minor background elements or artistic style unless it's the main subject.
- Do not begin with "This image shows..." or "The picture depicts...".
"""):
    # Resolve t.co URLs to actual image URLs
    actual_image_url = url
    if media_mappings and url in media_mappings:
        actual_image_url = media_mappings[url]
    elif url.startswith("https://t.co/") and not media_mappings:
        # If it's a t.co URL but we don't have mappings, we can't process it
        return f"ERROR: Cannot resolve t.co URL {url} without media mappings"
    
    # Download image with proper validation
    try:
        response = requests.get(actual_image_url, timeout=15)
        response.raise_for_status()  # Raise exception for 4xx/5xx status codes
        
        # Verify it's actually an image
        content_type = response.headers.get('Content-Type', '').lower()
        if not content_type.startswith('image/'):
            return f"ERROR: URL returned {content_type}, not an image"
        
        img_bytes = response.content
        if len(img_bytes) == 0:
            return "ERROR: Empty response from image URL"
            
    except requests.exceptions.RequestException as e:
        return f"ERROR: Failed to download image: {e}"
    
    img_hash = hashlib.sha1(img_bytes).hexdigest()
    
    # Check cache first if cache_dir provided
    if cache_dir:
        cache_file = cache_dir / f"{img_hash}.txt"
        if cache_file.exists():
            try:
                return cache_file.read_text(encoding="utf-8")
            except Exception as e:
                print(f"‚ö†Ô∏è  Failed to read cached caption for {img_hash}: {e}", file=sys.stderr)
                # Fall through to generate new caption
    
    # Generate new caption
    mime = mimetypes.guess_type(actual_image_url)[0] or "image/jpeg"
    client = get_client()
    resp = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=[
            types.Part.from_bytes(data=img_bytes, mime_type=mime),
            prompt
        ],
    )
    caption = resp.text
    
    # Save to cache if cache_dir provided
    if cache_dir:
        try:
            cache_dir.mkdir(exist_ok=True)
            cache_file = cache_dir / f"{img_hash}.txt"
            cache_file.write_text(caption, encoding="utf-8")
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to cache caption for {img_hash}: {e}")
    
    return caption


def generate_image_captions_from_texts(texts: List[str], media_mappings: Dict[str, str] = None, cache_dir: Path = None, max_images: int = 500) -> Dict[str, str]:
    """Generate captions for all images found in the given texts.
    
    Args:
        texts: List of text content to scan for image URLs
        media_mappings: Dictionary mapping t.co URLs to actual image URLs
        cache_dir: Optional directory for caching captions by image hash
        max_images: Maximum number of images to process (default 500)
    
    Returns:
        Dictionary mapping image URLs to their captions
    """
    url_to_caption = {}
    all_urls = set()
    
    # Collect all unique image URLs from all texts (regex grab direct pbs image/video thumb URLs)
    for text in texts:
        urls = IMG_RE.findall(text)
        all_urls.update(urls)
    
    # ALWAYS include short media URLs we learned from Entities
    if media_mappings:
        all_urls.update(media_mappings.keys())
    
    if not all_urls:
        return url_to_caption
    
    # Limit the number of images to process
    urls_to_process = list(all_urls)[:max_images]
    if len(all_urls) > max_images:
        print(f"‚ö†Ô∏è  Found {len(all_urls)} images, limiting to first {max_images} for processing")
        
    print(f"üñºÔ∏è  Processing {len(urls_to_process)} unique images for captioning")
    if cache_dir:
        print(f"üíæ  Using image caption cache: {cache_dir}")
    if media_mappings:
        print(f"üîó  Using media mappings for {len(media_mappings)} t.co URLs")
    
    # Generate captions for each unique URL
    for i, url in enumerate(urls_to_process, 1):
        # Skip t.co URLs that we don't have mappings for
        if url.startswith("https://t.co/") and (not media_mappings or url not in media_mappings):
            print(f"‚è≠Ô∏è  [{i}/{len(urls_to_process)}] Skipping unknown t.co URL: {url}")
            continue
            
        try:
            caption = describe_image(url, media_mappings, cache_dir)
            url_to_caption[url] = caption
            print(f"‚úÖ  [{i}/{len(urls_to_process)}] Generated caption: {url[:50]}...")
        except Exception as e:
            caption = f"ERROR: {e}"
            url_to_caption[url] = caption
            print(f"‚ùå  [{i}/{len(urls_to_process)}] Failed to caption {url[:50]}...: {e}")
    
    return url_to_caption


def generate_image_captions_from_urls(urls_to_process: List[str], cache_dir: Path = None, max_images: int = 500) -> Dict[str, str]:
    """Generate captions for a list of image URLs."""
    url_to_caption = {}
    if not urls_to_process:
        return url_to_caption

    # Limit the number of images to process
    if len(urls_to_process) > max_images:
        urls_to_process = urls_to_process[:max_images]
        print(f"‚ö†Ô∏è  Found {len(urls_to_process)} images, limiting to first {max_images} for processing")
        
    print(f"üñºÔ∏è  Processing {len(urls_to_process)} unique images for captioning")
    if cache_dir:
        print(f"üíæ  Using image caption cache: {cache_dir}")
    
    # Generate captions for each unique URL
    for i, url in enumerate(urls_to_process, 1):
        try:
            # For cleaned data, URLs are already expanded, no need for media_mappings
            caption = describe_image(url, None, cache_dir)
            url_to_caption[url] = caption
            print(f"‚úÖ  [{i}/{len(urls_to_process)}] Generated caption: {url[:50]}...")
        except Exception as e:
            caption = f"ERROR: {e}"
            url_to_caption[url] = caption
            print(f"‚ùå  [{i}/{len(urls_to_process)}] Failed to caption {url[:50]}...: {e}")
    
    return url_to_caption


def replace_images_with_captions(text: str, url_to_caption: Dict[str, str]) -> str:
    """Replace image URLs in text with [image](caption) format.
    If a caption is unavailable or failed to generate, a placeholder
    '[image](Image not captioned)' will be inserted instead.
    """
    import re

    def _repl(match: re.Match) -> str:
        url = match.group(0)
        caption = None
        if url_to_caption:
            caption = url_to_caption.get(url)
            if caption and caption.startswith("ERROR"):
                caption = None  # Treat error messages as missing captions
        if not caption:
            caption = "Image not captioned"
        safe_caption = caption.replace(")", r"\)").replace("\n", " ")
        return f"[image]({safe_caption})"

    # Apply replacement to every matched image URL (pbs.twimg.com, etc.)
    return IMG_RE.sub(_repl, text)


def save_captions_csv(url_to_caption: Dict[str, str], out_path: Path):
    """Save image URL to caption mappings as CSV, merging with existing data."""
    # Read existing captions if file exists
    existing_captions = {}
    if out_path.exists():
        try:
            with out_path.open("r", newline="", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    existing_captions[row["url"]] = row["caption"]
            print(f"üìñ  Loaded {len(existing_captions)} existing captions from {out_path.name}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to read existing captions: {e}")
    
    # Merge new captions with existing (new ones take priority)
    all_captions = existing_captions.copy()
    all_captions.update(url_to_caption)
    
    # Write merged captions
    captions = [{"url": url, "caption": caption} for url, caption in all_captions.items()]
    with out_path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, ["url", "caption"])
        writer.writeheader()
        writer.writerows(captions)
    
    new_count = len(url_to_caption)
    total_count = len(all_captions)
    print(f"‚úÖ  Saved {total_count} total captions ({new_count} new, {total_count - new_count} existing)")


def gen_captions(bookmarks_path: pathlib.Path, out_path: pathlib.Path):
    """Legacy function for backwards compatibility - generates captions from already-written file."""
    captions = []
    for line in bookmarks_path.read_text(encoding="utf-8").split("---"):
        for url in IMG_RE.findall(line):
            try:
                captions.append({"url": url,
                                 "caption": describe_image(url)})
            except Exception as e:
                captions.append({"url": url, "caption": f"ERROR: {e}"})
    # write a simple CSV the LLM can ingest
    with out_path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, ["url", "caption"])
        writer.writeheader()
        writer.writerows(captions)


# --------------------------------------------------------------------------- #
#  CLI                                                                        #
# --------------------------------------------------------------------------- #

def safe_messagebox(message_type: str, title: str, message: str) -> None:
    """Show message via GUI or print to console in headless environments."""
    try:
        if message_type == "error":
            messagebox.showerror(title, message)
        elif message_type == "info":
            messagebox.showinfo(title, message)
        else:
            messagebox.showinfo(title, message)
    except Exception:
        # Fallback to console output in headless environments
        print(f"{title}: {message}")


def get_folder_path() -> tuple[Path, argparse.Namespace]:
    """Get folder path via GUI or CLI arguments, with fallback for headless environments.
    
    Returns:
        Tuple of (folder_path, parsed_args)
    """
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Process Twitter data for LLM consumption")
    parser.add_argument("--folder", type=str, help="Path to folder containing Twitter data files")
    parser.add_argument("--self-handle", type=str, help="Your Twitter handle (without @) for self-identification")
    parser.add_argument("--self-id", type=str, help="Your Twitter user ID for self-identification")
    args = parser.parse_args()
    
    # If folder provided via CLI, use it
    if args.folder:
        folder = Path(args.folder)
        if not folder.exists() or not folder.is_dir():
            print(f"‚ùå  Invalid folder path: {folder}")
            sys.exit(1)
        print(f"üìÅ  Using folder from CLI: {folder}")
        return folder, args
    
    # Try GUI folder picker (may fail in headless environments)
    try:
        root = tk.Tk()
        root.withdraw()
        
        folder_path = filedialog.askdirectory(
            title="Select folder containing Twitter export files"
        )
        
        if not folder_path:
            print("‚ùå  No folder selected. Exiting.")
            sys.exit(1)
            
        folder = Path(folder_path)
        print(f"üìÅ  Selected folder: {folder}")
        return folder, args
        
    except tk.TclError as e:
        # Specific handling for display issues in headless environments
        print(f"‚ö†Ô∏è  GUI not available (no display): {e}")
        print("üí°  Use --folder argument to specify path, e.g.:")
        print(f"     python {sys.argv[0]} --folder /path/to/twitter/data")
    except Exception as e:
        # Fallback for other GUI issues
        print(f"‚ö†Ô∏è  GUI not available: {e}")
        print("üí°  Use --folder argument to specify path, e.g.:")
        print(f"     python {sys.argv[0]} --folder /path/to/twitter/data")
        
        # Prompt for input as last resort
        try:
            folder_input = input("üìÅ  Enter folder path: ").strip()
            if not folder_input:
                print("‚ùå  No folder provided. Exiting.")
                sys.exit(1)
                
            folder = Path(folder_input)
            if not folder.exists() or not folder.is_dir():
                print(f"‚ùå  Invalid folder path: {folder}")
                sys.exit(1)
                
            print(f"üìÅ  Using folder: {folder}")
            return folder, args
            
        except (KeyboardInterrupt, EOFError):
            print("\n‚ùå  Cancelled. Exiting.")
            sys.exit(1)


def determine_self_ids(args: argparse.Namespace, tweets_file: Path = None, **kwargs) -> set:
    """Determine user's own Twitter IDs from the new cleaned format."""
    self_ids = set()

    # 1. Check command line --self-id argument (highest priority)
    if args.self_id:
        self_ids.add(str(args.self_id))
        print(f"üÜî  Using self ID from command line: {args.self_id}")
        return self_ids

    # 2. Check command line --self-handle argument
    self_handle = args.self_handle
    
    # 3. Check environment variable MY_TWITTER_HANDLE if no CLI handle provided
    if not self_handle:
        self_handle = os.environ.get('MY_TWITTER_HANDLE')
        if self_handle:
            print(f"üÜî  Using self handle from environment variable: {self_handle}")

    # 4. Extract self ID from the cleaned tweets file
    if tweets_file and tweets_file.exists():
        print(f"üîç  Detecting self ID from {tweets_file.name}...")
        try:
            with tweets_file.open('r', encoding='utf-8') as infile:
                for line in infile:
                    if not line.strip():
                        continue
                    try:
                        # Read the author_id from the new clean format
                        obj = json.loads(line)
                        uid = obj.get("author_id")
                        if uid:
                            self_ids.add(str(uid))
                    except Exception:
                        continue
            if self_ids:
                print(f"‚úÖ  Found {len(self_ids)} self ID(s) from tweets file: {self_ids}")
                if len(self_ids) > 1:
                    print(f"‚ö†Ô∏è  Multiple candidate self IDs detected: {sorted(self_ids)}. "
                          "Use --self-id to disambiguate.")
                return self_ids
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to read tweets file for self ID detection: {e}")

    # 5. Fallback warnings
    if self_handle and not self_ids:
        print(f"‚ö†Ô∏è  Handle '{self_handle}' provided but no tweets file available for ID lookup.")
        print("üí°  Use --self-id to provide your Twitter user ID directly, or include a cleaned_tweets_*.jsonl file.")
    
    if not self_ids:
        print("‚ö†Ô∏è  Could not determine self ID. Use --self-handle or --self-id for accurate self-identification.")
        print("üí°  Your tweets will not be labeled as '@me' in output files.")

    return self_ids








def main() -> None:
    folder, args = get_folder_path()
    
    # Find the required files
    tweets_file, likes_file, bookmarks_file, replies_file, parents_file = find_files_in_folder(folder)
    
    # Determine self IDs from various sources
    self_ids = determine_self_ids(args, tweets_file)
    
    # Check what files we found
    found_files = []
    missing_files = []
    
    if tweets_file:
        found_files.append(f"‚úÖ  Found tweets file: {tweets_file.name}")
    else:
        missing_files.append("‚ö†Ô∏è  No cleaned_tweets_*.jsonl file found")
    
    if likes_file:
        found_files.append(f"‚úÖ  Found likes file: {likes_file.name}")
    else:
        missing_files.append("‚ö†Ô∏è  No cleaned_likes_*.jsonl file found")
    
    if bookmarks_file:
        found_files.append(f"‚úÖ  Found bookmarks file: {bookmarks_file.name}")
    else:
        missing_files.append("‚ö†Ô∏è  No cleaned_bookmarks_*.jsonl file found")
    
    if replies_file:
        found_files.append(f"‚úÖ  Found replies file: {replies_file.name}")
    else:
        missing_files.append("‚ö†Ô∏è  No cleaned_replies_*.jsonl file found")
    
    if parents_file:
        found_files.append(f"‚úÖ  Found parents file: {parents_file.name}")
    else:
        missing_files.append("‚ÑπÔ∏è  No parents.json file found (optional - run hydrate_parents_api.py first to enable reply context)")
    
    # Print status
    for msg in found_files:
        print(msg)
    for msg in missing_files:
        print(msg)
    
    # Print image captioning settings
    print("\nüì∏  Image captioning settings:")
    print(f"     Tweets: {'‚úÖ Enabled' if ENABLE_CAPTIONS_TWEETS else '‚ùå Disabled'}")
    print(f"     Likes: {'‚úÖ Enabled' if ENABLE_CAPTIONS_LIKES else '‚ùå Disabled'}")
    print(f"     Bookmarks: {'‚úÖ Enabled' if ENABLE_CAPTIONS_BOOKMARKS else '‚ùå Disabled'}")
    print(f"     Replies: {'‚úÖ Enabled' if ENABLE_CAPTIONS_REPLIES else '‚ùå Disabled'}")
    
    # Check if we have at least one data file
    if not tweets_file and not replies_file and not likes_file and not bookmarks_file:
        error_msg = "Cannot proceed without at least one cleaned file. Run hydrate_parents_api.py first to generate cleaned_*.jsonl files."
        print(f"‚ùå  {error_msg}")
        safe_messagebox("error", "Missing Files", error_msg)
        return
    
    # Process data using unified approach with de-duplication
    try:
        all_records = []  # Combined records from all sources
        all_text_lookups = {}  # Combined text lookups with de-duplication priority: tweets > bookmarks > likes > parents
        all_media_mappings = {}  # Collect media mappings from all sources
        all_url_mappings = {}
        all_meta_by_id = {}  # Combined metadata for relationship tracking
        
        # Process all files with unified parser - tweets get highest priority
        if tweets_file:
            print("\nüîÑ  Processing tweets...")
            tweets_records, tweets_lookup, tweets_media, tweets_urls, tweets_meta = parse_twitter_jsonl(tweets_file, "tweets", self_ids)
            all_records.extend(tweets_records)
            all_text_lookups.update(tweets_lookup)  # Tweets get first priority
            all_media_mappings.update(tweets_media)
            all_url_mappings.update(tweets_urls)
            all_meta_by_id.update(tweets_meta)
        
        if likes_file:
            print("üîÑ  Processing likes...")
            likes_records, likes_lookup, likes_media, likes_urls, likes_meta = parse_twitter_jsonl(likes_file, "likes", self_ids)
            all_records.extend(likes_records)
            # Add likes to lookup, but don't overwrite tweets
            for tweet_id, text in likes_lookup.items():
                if tweet_id not in all_text_lookups:
                    all_text_lookups[tweet_id] = text
            # Merge metadata, but don't overwrite tweets
            for tweet_id, meta in likes_meta.items():
                if tweet_id not in all_meta_by_id:
                    all_meta_by_id[tweet_id] = meta
            all_media_mappings.update(likes_media)
            all_url_mappings.update(likes_urls)
        
        if bookmarks_file:
            print("üîÑ  Processing bookmarks...")
            bookmarks_records, bookmarks_lookup, bookmarks_media, bookmarks_urls, bookmarks_meta = parse_twitter_jsonl(bookmarks_file, "bookmarks", self_ids)
            all_records.extend(bookmarks_records)
            # Add bookmarks to lookup, but don't overwrite tweets or likes  
            for tweet_id, text in bookmarks_lookup.items():
                if tweet_id not in all_text_lookups:
                    all_text_lookups[tweet_id] = text
            # Merge metadata, but don't overwrite tweets or likes
            for tweet_id, meta in bookmarks_meta.items():
                if tweet_id not in all_meta_by_id:
                    all_meta_by_id[tweet_id] = meta
            all_media_mappings.update(bookmarks_media)
            all_url_mappings.update(bookmarks_urls)
        
        if replies_file:
            print("üîÑ  Processing replies...")
            replies_records, replies_lookup, replies_media, replies_urls, replies_meta = \
                parse_twitter_jsonl(replies_file, "replies", self_ids)

            all_records.extend(replies_records)
            for tweet_id, text in replies_lookup.items():
                if tweet_id not in all_text_lookups:
                    all_text_lookups[tweet_id] = text
            all_media_mappings.update(replies_media)
            all_url_mappings.update(replies_urls)
            
            # Change this line:
            # all_meta_by_id.update(replies_meta)
            
            # To this loop:
            for tweet_id, meta in replies_meta.items():
                if tweet_id not in all_meta_by_id:
                    all_meta_by_id[tweet_id] = meta
        
        # Load parent tweets if available
        if parents_file:
            print("üîÑ  Loading parent tweets...")
            parent_lookup, parent_url_mappings, parent_metadata = load_parents_json(parents_file)
            all_url_mappings.update(parent_url_mappings)
            # Add parents to lookup, but don't overwrite existing tweets
            parents_added = 0
            for tweet_id, text in parent_lookup.items():
                if tweet_id not in all_text_lookups:
                    all_text_lookups[tweet_id] = text
                    parents_added += 1
            
            # Merge parent metadata, but don't overwrite existing
            parents_meta_added = 0
            for tweet_id, meta in parent_metadata.items():
                if tweet_id not in all_meta_by_id:
                    all_meta_by_id[tweet_id] = meta
                    parents_meta_added += 1
            
            print(f"üìñ  Added {parents_added} parent tweets to lookup (total in parents.json: {len(parent_lookup)})")
            if parents_meta_added > 0:
                print(f"üîó  Added {parents_meta_added} parent metadata entries for deeper context chains")
        
        print(f"üìñ  Total unique tweets in lookup: {len(all_text_lookups)} (with de-duplication priority: tweets > replies > bookmarks > likes > parents)")
        
        # Check for reply/quote context availability across all sources
        all_replies = sum(1 for r in all_records if r.get("is_reply"))
        all_quotes = sum(1 for r in all_records if r.get("quoted_tweet_id"))
        tweets_with_replies = sum(1 for r in all_records if r.get("source") == "tweets" and r.get("is_reply"))
        
        print(f"üîó  Relationship detection: {all_replies} replies, {all_quotes} quotes across all sources")
        print(f"üìã  Metadata tracking: {len(all_meta_by_id)} tweets with relationship metadata")
        
        if tweets_with_replies > 0:
            if not parents_file:
                print(f"üí°  Found {tweets_with_replies} reply tweets, but no parent context available. Run 'python hydrate_parents_api.py' first to see original tweets in reply chains.")
            else:
                print(f"üîó  Found {tweets_with_replies} reply tweets with parent context available")
        
        # --- NEW URL & MEDIA HANDLING ---
        print("\nüîÑ  Collecting all URLs and Media for processing...")
        all_external_urls = set()
        all_media_urls = set()
        for record in all_records:
            all_external_urls.update(record.get('urls', []))
            all_media_urls.update(record.get('media_urls', []))

        # Generate image captions for enabled file types only
        url_to_caption = {}
        if any([ENABLE_CAPTIONS_TWEETS, ENABLE_CAPTIONS_LIKES, ENABLE_CAPTIONS_BOOKMARKS, ENABLE_CAPTIONS_REPLIES]):
            try:
                print(f"üîÑ  Generating image captions for {len(all_media_urls)} media items...")
                image_cache_dir = folder / "image_cache"
                # This function needs a small change to accept a list of URLs directly
                url_to_caption = generate_image_captions_from_urls(list(all_media_urls), image_cache_dir, 1000)
                if url_to_caption:
                    save_captions_csv(url_to_caption, folder / "image_captions.csv")
                    print("‚úÖ  Generated image captions and exported image_captions.csv")
                else:
                    print("‚ÑπÔ∏è  No images found in the records")
            except Exception as e:
                print(f"‚ö†Ô∏è  Failed generating image captions: {e}")

        # Generate URL metadata for all texts
        url_to_meta = {}
        try:
            print(f"üîÑ  Generating URL metadata for {len(all_external_urls)} URLs...")
            # This function also needs to be adapted to take a list of URLs
            url_to_meta = generate_url_metadata_from_urls(list(all_external_urls), 3500)
            if url_to_meta:
                save_url_metadata_csv(url_to_meta, folder / "url_metadata.csv")
                print("‚úÖ  Generated URL metadata and exported url_metadata.csv")
            else:
                print("‚ÑπÔ∏è  No external URLs found in the records")
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed generating URL metadata: {e}")
        
        # Export text files using unified approach with proper deduplication
        # Deduplicate records by tweet_id with priority: tweets/replies > bookmarks > likes
        seen_tweet_ids = set()
        combined_tweets_and_replies_records = []
        likes_records = []
        bookmarks_records = []
        
        # First pass: tweets and replies (highest priority, combined)
        for r in all_records:
            tweet_id = r.get("id")
            if r["source"] in ("tweets", "replies") and tweet_id and tweet_id not in seen_tweet_ids:
                combined_tweets_and_replies_records.append(r)
                seen_tweet_ids.add(tweet_id)
        
        # Second pass: bookmarks (medium priority)
        for r in all_records:
            tweet_id = r.get("id")
            if r["source"] == "bookmarks" and tweet_id and tweet_id not in seen_tweet_ids:
                bookmarks_records.append(r)
                seen_tweet_ids.add(tweet_id)
        
        # Third pass: likes (lowest priority)
        for r in all_records:
            tweet_id = r.get("id")
            if r["source"] == "likes" and tweet_id and tweet_id not in seen_tweet_ids:
                likes_records.append(r)
                seen_tweet_ids.add(tweet_id)
        
        # Sort the combined tweets and replies by date (newest first)
        # This is crucial to maintain chronological order in the final output file.
        if combined_tweets_and_replies_records:
            print("üîÑ  Sorting combined tweets and replies chronologically...")
            combined_tweets_and_replies_records.sort(
                key=lambda r: datetime.strptime(r['created_at'], '%a %b %d %H:%M:%S %z %Y'),
                reverse=True
            )
        
        print(f"üìä  After deduplication and combination: "
              f"{len(combined_tweets_and_replies_records)} tweets/replies, "
              f"{len(bookmarks_records)} bookmarks, "
              f"{len(likes_records)} likes")
        
        # Export the combined tweets and replies file
        if combined_tweets_and_replies_records:
            # Captions for tweets and replies are enabled/disabled by the same flags
            tweets_replies_captions = url_to_caption if (ENABLE_CAPTIONS_TWEETS or ENABLE_CAPTIONS_REPLIES) else {}
            export_unified_text(
                combined_tweets_and_replies_records,
                all_text_lookups,
                folder / "tweets_for_llm.txt", # Output to the combined file
                all_meta_by_id,
                tweets_replies_captions,
                url_to_meta,
                all_url_mappings
            )
            print("‚úÖ  Exported combined tweets_for_llm.txt")
        
        # Export likes and bookmarks as before
        if likes_records:
            likes_captions = url_to_caption if ENABLE_CAPTIONS_LIKES else {}
            export_unified_text(likes_records, all_text_lookups, folder / "likes_for_llm.txt", all_meta_by_id, likes_captions, url_to_meta, all_url_mappings)
            print("‚úÖ  Exported likes_for_llm.txt")
        
        if bookmarks_records:
            bookmarks_captions = url_to_caption if ENABLE_CAPTIONS_BOOKMARKS else {}
            export_unified_text(bookmarks_records, all_text_lookups, folder / "bookmarks_for_llm.txt", all_meta_by_id, bookmarks_captions, url_to_meta, all_url_mappings)
            print("‚úÖ  Exported bookmarks_for_llm.txt")
            
    except Exception as e:
        error_msg = f"Failed processing files: {e}"
        print(f"‚ùå  {error_msg}")
        safe_messagebox("error", "Processing Error", error_msg)
        return

    success_msg = f"‚úÖ  Done! Output files written to:\n{folder}"
    print(f"\n{success_msg}")
    safe_messagebox("info", "Success", success_msg)


if __name__ == "__main__":  # pragma: no cover
    main()
