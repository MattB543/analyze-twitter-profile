A screenshot of an article by John David Pressman, dated 2025-06-24, titled "Why Aren't LLMs General Intelligence Yet?". The article poses why large language models, despite excelling at "predict the next item in the sequence," lack the "act so as to profit from experience" aspect of intelligence central to Legg and Hutter's Universal Intelligence, questioning if "perfecting prediction of the next token just gets you an ever more brightly polished mediocre mind?" It identifies three main bottlenecks preventing LLM agents from achieving general intelligence: "General Action Space," which the author argues is not the issue, citing frameworks like Tan et al's Cradle and tools like Voyager, Cradle, SmolAgents, and Weave-Agent; "Lifetime Learning/Test Time Training/Corpus Expansion/etc," suggested as a likely bottleneck due to the lack of consensus on methods like Self Adapting Language Models or those used by Google Gemini models; and "Short, Medium and Long Term Memory," which details challenges with managing context windows and retrieval-augmented generation (RAG).