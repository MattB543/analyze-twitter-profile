This infographic, "Hundreds of models have been trained with over 10^23 FLOP," is a scatter plot from EPOCH AI visualizing the training compute (FLOP) of numerous AI models against their publication dates from 2018 to 2025. The central message is the dramatic and exponential increase in computational resources required for training cutting-edge AI models over time, with many models surpassing 10^23 FLOP and projected to reach 10^26 FLOP by 2025. The data, comprising 241 results, highlights that the United States and China are the primary contributors to these high-compute models, with significant contributions from multinational entities and other countries. Key models displayed include AlphaGo Zero, GPT-3 175B, PaLM, LLaMA-65B, Gemini 1.0 Ultra, GPT-4, Grok-2, and projected future models like Grok-3 and GPT-4.5.